{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d281536-0ccd-478c-a9b5-9daf67215299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in e:\\conda-forge\\lib\\site-packages (3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "403d4f9c-c270-426c-8d81-c3cc8b627189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81a62cdf-9f67-46fa-87b9-1039688eb8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [............................................................................] 405618 / 405618File downloaded as pg66944 (2).txt\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "\"\"\"\n",
    "Importing (and hence utilising) the wget function to pull data from a file.\n",
    "Which in this case will be the dataset that this Generative Pre-Training Transformer\n",
    "will rely on.\n",
    "\n",
    "This is the 'The Origin and Development of the Quantum Theory' by Max Planck, obtained\n",
    "via the Project Gutenberg website.\n",
    "\"\"\"\n",
    "\n",
    "# Download a file\n",
    "url = \"https://raw.githubusercontent.com/Utartizan/Quantum-Theory-GPT/refs/heads/main/pg66944.txt\"\n",
    "filename = wget.download(url)\n",
    "\n",
    "print(f\"File downloaded as {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e98e6f-65ec-4962-8c06-152e5652bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pg66944.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "505d818e-08ad-4c12-abdf-021edf6190ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters is:  391617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOutputs the amount of characters within the attached dataset. \\nIn this case, usually the longer the length of the dataset the\\nbetter (regarding accuracy) the generative material is.\\n\\nThis is due to the quantity in training and validation data that the\\nmodel can use.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"length of dataset in characters is: \", len(text))\n",
    "\n",
    "\"\"\"\n",
    "Outputs the amount of characters within the attached dataset. \n",
    "In this case, usually the longer the length of the dataset the\n",
    "better (regarding accuracy) the generative material is.\n",
    "\n",
    "This is due to the quantity in training and validation data that the\n",
    "model can use.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b891a07b-d6cb-4e59-820d-94583c314a2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿Lord Kelvin writing in 1893, in his preface to the English edition of\n",
      "Hertz’s Researches on Electric Waves, says “many workers and many\n",
      "thinkers have helped to build up the nineteenth century school of\n",
      "_plenum_, one ether for light, heat, electricity, magnetism; and the\n",
      "German and English volumes containing Hertz’s electrical papers, given\n",
      "to the world in the last decade of the century, will be a permanent\n",
      "monument of the splendid consummation now realised.”\n",
      "\n",
      "Ten years later, in 1905, we find Einstein declaring that “the ether\n",
      "will be proved to be superfluous.” At first sight the revolution in\n",
      "scientific thought brought about in the course of a single decade\n",
      "appears to be almost too violent. A more careful even though a rapid\n",
      "review of the subject will, however, show how the Theory of Relativity\n",
      "gradually became a historical necessity.\n",
      "\n",
      "Towards the beginning of the nineteenth century, the luminiferous ether\n",
      "came into prominence as a result of the brilliant successes of the wave\n",
      "theory\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62749558-7ef8-4fc6-ac82-cc7afb9274fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\n",
      " $&'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_abcdefghijklmnopqrstuvwxyz{|}§°±²³´·¹½Ä×äæéôöüĀĊūΓΔΘΣΤΦΨΩαβγδεζηθκλμνξπρστφχψωḂḞḟṠṡṽẇῶ—‘’“”′″‴⁰⁴⁵⁷⁸⁻₀₁₂₃₄∂∑√∞∫∴≠□▽﻿\n",
      "176\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Scan the entire file (text) and output:\n",
    "1. All the types of characters utilised\n",
    "2. The quantifiable size of the characters utilised (94)\n",
    "\"\"\"\n",
    "\n",
    "characters = sorted(list(set(text)))\n",
    "vocabularySize = len(characters)\n",
    "print(''.join(characters))\n",
    "print(vocabularySize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45cb1237-c362-4f61-8f04-06d2a4f7824d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 81, 61, 74, 80, 81, 73, 2, 80, 68, 65, 75, 78, 85, 10, 2, 75, 78, 2, 77, 81, 61, 74, 80, 81, 73, 2, 73, 65, 63, 68, 61, 74, 69, 63, 79, 10, 2, 69, 79, 2, 61, 2, 66, 81, 74, 64, 61, 73, 65, 74, 80, 61, 72, 2, 80, 68, 65, 75, 78, 85, 2, 69, 74, 2, 76, 68, 85, 79, 69, 63, 79, 2, 80, 68, 61, 80, 2, 64, 65, 79, 63, 78, 69, 62, 65, 79, 2, 80, 68, 65, 2, 62, 65, 68, 61, 82, 69, 75, 78, 2, 75, 66, 2, 73, 61, 80, 80, 65, 78, 2, 61, 74, 64, 2, 65, 74, 65, 78, 67, 85, 2, 61, 80, 2, 80, 68, 65, 2, 79, 73, 61, 72, 72, 65, 79, 80, 2, 79, 63, 61, 72, 65, 79, 10, 2, 72, 69, 71, 65, 2, 61, 80, 75, 73, 79, 2, 61, 74, 64, 2, 79, 81, 62, 61, 80, 75, 73, 69, 63, 2, 76, 61, 78, 80, 69, 63, 72, 65, 79, 12]\n",
      "Quantum theory, or quantum mechanics, is a fundamental theory in physics that describes the behavior of matter and energy at the smallest scales, like atoms and subatomic particles.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implementing encoding and decoding functions for a character-level tokeniser\n",
    "\n",
    "This essentially assigns each character to its assigned number, for every character\n",
    "in the list is its own number.\n",
    "\n",
    "In this case, the value 50 belongs to the character Q\n",
    "the value 85 belongs to the character U\n",
    "the value 1 belongs to the space character(?)\n",
    "\n",
    "The print functions below exercises the model's ability to both encode and decode\n",
    "a set of charaters, in the form of many words, in the form of a singular sentence \n",
    "accordingly.\n",
    "\"\"\"\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(characters) }\n",
    "itos = { i:ch for i,ch in enumerate(characters) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"Quantum theory, or quantum mechanics, is a fundamental theory in physics that describes the behavior of matter and energy at the smallest scales, like atoms and subatomic particles.\"))\n",
    "print(decode(encode(\"Quantum theory, or quantum mechanics, is a fundamental theory in physics that describes the behavior of matter and energy at the smallest scales, like atoms and subatomic particles.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de39bf0d-05d8-44d3-a078-07b7d151d52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in e:\\conda-forge\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in e:\\conda-forge\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in e:\\conda-forge\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in e:\\conda-forge\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in e:\\conda-forge\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in e:\\conda-forge\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in e:\\conda-forge\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in e:\\conda-forge\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\conda-forge\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\conda-forge\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a4828e4-f57d-425a-bcc9-592facd6fed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([391617]) torch.int64\n",
      "tensor([175,  41,  75,  78,  64,   2,  40,  65,  72,  82,  69,  74,   2,  83,\n",
      "         78,  69,  80,  69,  74,  67,   2,  69,  74,   2,  15,  22,  23,  17,\n",
      "         10,   2,  69,  74,   2,  68,  69,  79,   2,  76,  78,  65,  66,  61,\n",
      "         63,  65,   2,  80,  75,   2,  80,  68,  65,   2,  34,  74,  67,  72,\n",
      "         69,  79,  68,   2,  65,  64,  69,  80,  69,  75,  74,   2,  75,  66,\n",
      "          1,  37,  65,  78,  80,  86, 149,  79,   2,  47,  65,  79,  65,  61,\n",
      "         78,  63,  68,  65,  79,   2,  75,  74,   2,  34,  72,  65,  63,  80,\n",
      "         78,  69,  63,   2,  52,  61,  82,  65,  79,  10,   2,  79,  61,  85,\n",
      "         79,   2, 150,  73,  61,  74,  85,   2,  83,  75,  78,  71,  65,  78,\n",
      "         79,   2,  61,  74,  64,   2,  73,  61,  74,  85,   1,  80,  68,  69,\n",
      "         74,  71,  65,  78,  79,   2,  68,  61,  82,  65,   2,  68,  65,  72,\n",
      "         76,  65,  64,   2,  80,  75,   2,  62,  81,  69,  72,  64,   2,  81,\n",
      "         76,   2,  80,  68,  65,   2,  74,  69,  74,  65,  80,  65,  65,  74,\n",
      "         80,  68,   2,  63,  65,  74,  80,  81,  78,  85,   2,  79,  63,  68,\n",
      "         75,  75,  72,   2,  75,  66,   1,  60,  76,  72,  65,  74,  81,  73,\n",
      "         60,  10,   2,  75,  74,  65,   2,  65,  80,  68,  65,  78,   2,  66,\n",
      "         75,  78,   2,  72,  69,  67,  68,  80,  10,   2,  68,  65,  61,  80,\n",
      "         10,   2,  65,  72,  65,  63,  80,  78,  69,  63,  69,  80,  85,  10,\n",
      "          2,  73,  61,  67,  74,  65,  80,  69,  79,  73,  25,   2,  61,  74,\n",
      "         64,   2,  80,  68,  65,   1,  36,  65,  78,  73,  61,  74,   2,  61,\n",
      "         74,  64,   2,  34,  74,  67,  72,  69,  79,  68,   2,  82,  75,  72,\n",
      "         81,  73,  65,  79,   2,  63,  75,  74,  80,  61,  69,  74,  69,  74,\n",
      "         67,   2,  37,  65,  78,  80,  86, 149,  79,   2,  65,  72,  65,  63,\n",
      "         80,  78,  69,  63,  61,  72,   2,  76,  61,  76,  65,  78,  79,  10,\n",
      "          2,  67,  69,  82,  65,  74,   1,  80,  75,   2,  80,  68,  65,   2,\n",
      "         83,  75,  78,  72,  64,   2,  69,  74,   2,  80,  68,  65,   2,  72,\n",
      "         61,  79,  80,   2,  64,  65,  63,  61,  64,  65,   2,  75,  66,   2,\n",
      "         80,  68,  65,   2,  63,  65,  74,  80,  81,  78,  85,  10,   2,  83,\n",
      "         69,  72,  72,   2,  62,  65,   2,  61,   2,  76,  65,  78,  73,  61,\n",
      "         74,  65,  74,  80,   1,  73,  75,  74,  81,  73,  65,  74,  80,   2,\n",
      "         75,  66,   2,  80,  68,  65,   2,  79,  76,  72,  65,  74,  64,  69,\n",
      "         64,   2,  63,  75,  74,  79,  81,  73,  73,  61,  80,  69,  75,  74,\n",
      "          2,  74,  75,  83,   2,  78,  65,  61,  72,  69,  79,  65,  64,  12,\n",
      "        151,   1,   1,  49,  65,  74,   2,  85,  65,  61,  78,  79,   2,  72,\n",
      "         61,  80,  65,  78,  10,   2,  69,  74,   2,  15,  23,  14,  19,  10,\n",
      "          2,  83,  65,   2,  66,  69,  74,  64,   2,  34,  69,  74,  79,  80,\n",
      "         65,  69,  74,   2,  64,  65,  63,  72,  61,  78,  69,  74,  67,   2,\n",
      "         80,  68,  61,  80,   2, 150,  80,  68,  65,   2,  65,  80,  68,  65,\n",
      "         78,   1,  83,  69,  72,  72,   2,  62,  65,   2,  76,  78,  75,  82,\n",
      "         65,  64,   2,  80,  75,   2,  62,  65,   2,  79,  81,  76,  65,  78,\n",
      "         66,  72,  81,  75,  81,  79,  12, 151,   2,  30,  80,   2,  66,  69,\n",
      "         78,  79,  80,   2,  79,  69,  67,  68,  80,   2,  80,  68,  65,   2,\n",
      "         78,  65,  82,  75,  72,  81,  80,  69,  75,  74,   2,  69,  74,   1,\n",
      "         79,  63,  69,  65,  74,  80,  69,  66,  69,  63,   2,  80,  68,  75,\n",
      "         81,  67,  68,  80,   2,  62,  78,  75,  81,  67,  68,  80,   2,  61,\n",
      "         62,  75,  81,  80,   2,  69,  74,   2,  80,  68,  65,   2,  63,  75,\n",
      "         81,  78,  79,  65,   2,  75,  66,   2,  61,   2,  79,  69,  74,  67,\n",
      "         72,  65,   2,  64,  65,  63,  61,  64,  65,   1,  61,  76,  76,  65,\n",
      "         61,  78,  79,   2,  80,  75,   2,  62,  65,   2,  61,  72,  73,  75,\n",
      "         79,  80,   2,  80,  75,  75,   2,  82,  69,  75,  72,  65,  74,  80,\n",
      "         12,   2,  30,   2,  73,  75,  78,  65,   2,  63,  61,  78,  65,  66,\n",
      "         81,  72,   2,  65,  82,  65,  74,   2,  80,  68,  75,  81,  67,  68,\n",
      "          2,  61,   2,  78,  61,  76,  69,  64,   1,  78,  65,  82,  69,  65,\n",
      "         83,   2,  75,  66,   2,  80,  68,  65,   2,  79,  81,  62,  70,  65,\n",
      "         63,  80,   2,  83,  69,  72,  72,  10,   2,  68,  75,  83,  65,  82,\n",
      "         65,  78,  10,   2,  79,  68,  75,  83,   2,  68,  75,  83,   2,  80,\n",
      "         68,  65,   2,  49,  68,  65,  75,  78,  85,   2,  75,  66,   2,  47,\n",
      "         65,  72,  61,  80,  69,  82,  69,  80,  85,   1,  67,  78,  61,  64,\n",
      "         81,  61,  72,  72,  85,   2,  62,  65,  63,  61,  73,  65,   2,  61,\n",
      "          2,  68,  69,  79,  80,  75,  78,  69,  63,  61,  72,   2,  74,  65,\n",
      "         63,  65,  79,  79,  69,  80,  85,  12,   1,   1,  49,  75,  83,  61,\n",
      "         78,  64,  79,   2,  80,  68,  65,   2,  62,  65,  67,  69,  74,  74,\n",
      "         69,  74,  67,   2,  75,  66,   2,  80,  68,  65,   2,  74,  69,  74,\n",
      "         65,  80,  65,  65,  74,  80,  68,   2,  63,  65,  74,  80,  81,  78,\n",
      "         85,  10,   2,  80,  68,  65,   2,  72,  81,  73,  69,  74,  69,  66,\n",
      "         65,  78,  75,  81,  79,   2,  65,  80,  68,  65,  78,   1,  63,  61,\n",
      "         73,  65,   2,  69,  74,  80,  75,   2,  76,  78,  75,  73,  69,  74,\n",
      "         65,  74,  63,  65,   2,  61,  79,   2,  61,   2,  78,  65,  79,  81,\n",
      "         72,  80,   2,  75,  66,   2,  80,  68,  65,   2,  62,  78,  69,  72,\n",
      "         72,  69,  61,  74,  80,   2,  79,  81,  63,  63,  65,  79,  79,  65,\n",
      "         79,   2,  75,  66,   2,  80,  68,  65,   2,  83,  61,  82,  65,   1,\n",
      "         80,  68,  65,  75,  78,  85])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2e17c52-d118-481b-8915-26e81a0d8ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "trainingData = data[:n]\n",
    "validationData = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e8af985-07dc-489a-bf39-c4f4bafbc18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([175,  41,  75,  78,  64,   2,  40,  65,  72])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "trainingData[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a938998-415e-44ea-87ab-3a3ac770a24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([175]) the target: 41\n",
      "when input is tensor([175,  41]) the target: 75\n",
      "when input is tensor([175,  41,  75]) the target: 78\n",
      "when input is tensor([175,  41,  75,  78]) the target: 64\n",
      "when input is tensor([175,  41,  75,  78,  64]) the target: 2\n",
      "when input is tensor([175,  41,  75,  78,  64,   2]) the target: 40\n",
      "when input is tensor([175,  41,  75,  78,  64,   2,  40]) the target: 65\n",
      "when input is tensor([175,  41,  75,  78,  64,   2,  40,  65]) the target: 72\n"
     ]
    }
   ],
   "source": [
    "x = trainingData[:block_size]\n",
    "y = trainingData[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52684261-f666-4613-999d-1dbbde268b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[69, 78, 79, 80,  2, 80, 68, 65],\n",
      "        [ 2, 78, 65, 72, 61, 80, 69, 82],\n",
      "        [ 2, 80, 68, 65,  2, 65, 77, 81],\n",
      "        [65, 78, 61, 62, 72, 65,  2, 73]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[78, 79, 80,  2, 80, 68, 65, 78],\n",
      "        [78, 65, 72, 61, 80, 69, 82, 69],\n",
      "        [80, 68, 65,  2, 65, 77, 81, 61],\n",
      "        [78, 61, 62, 72, 65,  2, 73, 61]])\n",
      "----\n",
      "when input is [69] the target: 78\n",
      "when input is [69, 78] the target: 79\n",
      "when input is [69, 78, 79] the target: 80\n",
      "when input is [69, 78, 79, 80] the target: 2\n",
      "when input is [69, 78, 79, 80, 2] the target: 80\n",
      "when input is [69, 78, 79, 80, 2, 80] the target: 68\n",
      "when input is [69, 78, 79, 80, 2, 80, 68] the target: 65\n",
      "when input is [69, 78, 79, 80, 2, 80, 68, 65] the target: 78\n",
      "when input is [2] the target: 78\n",
      "when input is [2, 78] the target: 65\n",
      "when input is [2, 78, 65] the target: 72\n",
      "when input is [2, 78, 65, 72] the target: 61\n",
      "when input is [2, 78, 65, 72, 61] the target: 80\n",
      "when input is [2, 78, 65, 72, 61, 80] the target: 69\n",
      "when input is [2, 78, 65, 72, 61, 80, 69] the target: 82\n",
      "when input is [2, 78, 65, 72, 61, 80, 69, 82] the target: 69\n",
      "when input is [2] the target: 80\n",
      "when input is [2, 80] the target: 68\n",
      "when input is [2, 80, 68] the target: 65\n",
      "when input is [2, 80, 68, 65] the target: 2\n",
      "when input is [2, 80, 68, 65, 2] the target: 65\n",
      "when input is [2, 80, 68, 65, 2, 65] the target: 77\n",
      "when input is [2, 80, 68, 65, 2, 65, 77] the target: 81\n",
      "when input is [2, 80, 68, 65, 2, 65, 77, 81] the target: 61\n",
      "when input is [65] the target: 78\n",
      "when input is [65, 78] the target: 61\n",
      "when input is [65, 78, 61] the target: 62\n",
      "when input is [65, 78, 61, 62] the target: 72\n",
      "when input is [65, 78, 61, 62, 72] the target: 65\n",
      "when input is [65, 78, 61, 62, 72, 65] the target: 2\n",
      "when input is [65, 78, 61, 62, 72, 65, 2] the target: 73\n",
      "when input is [65, 78, 61, 62, 72, 65, 2, 73] the target: 61\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This batch of code illustrates how sequences are batched \n",
    "and how each prediction is made based on context.\n",
    "\n",
    "For each sequence in the batch, we're looking at how the model would predict\n",
    "the next character/token on what it has gone through so far.\n",
    "\n",
    "This would allow for the model to train (learn the dependencies in sequences).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "torch.manual_seed(6969) # setting a completely random (lol funny i know) number to allow for reproducibility\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions? how many elements used to predict the next?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y for the model\n",
    "    data = trainingData if split == 'train' else validationData\n",
    "    #It chooses between trainingData or validationData based on the split parameter.\n",
    "    \n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \"\"\"\n",
    "    Generates random starting indices for data samples.\n",
    "    The subtraction of block_size ensures that there's enough data for each sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    \n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "588b6b5f-77f8-4df7-9d5c-3c479deb4eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[69, 78, 79, 80,  2, 80, 68, 65],\n",
      "        [ 2, 78, 65, 72, 61, 80, 69, 82],\n",
      "        [ 2, 80, 68, 65,  2, 65, 77, 81],\n",
      "        [65, 78, 61, 62, 72, 65,  2, 73]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9b6624a-6c7b-4ada-9f63-d3070a2f4369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 176])\n",
      "tensor(5.5570, grad_fn=<NllLossBackward0>)\n",
      "\bλ&b3?⁻ô∞xḂxge√&√ξΣ²φn▽g′θBβt₄s³Eα-JKsψV₀ḂR²“;,psΨU5CJö(7ḟ∑Tη9üfΤẇHuD,{▽4du&4γcĀq.RQρ⁴N$8p·“φ⁰ä5′Yyε9\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(6969)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Creating the definition for the Model (BLModel), essentially creating an embedding\n",
    "layer where each token is assigned/mapped to a vector of the same identity as the\n",
    "vocabulary.\n",
    "\n",
    "This is to allow each token to represent a learnable table where each entry \n",
    "will correspond to X logits for the corresponding token.\n",
    "\"\"\"\n",
    "\n",
    "class BLModel(nn.Module):\n",
    "    def __init__(self, vocabularySize):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocabularySize, vocabularySize)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Logits are unnormalised predictions that would be output by this model\n",
    "        for each class in a classification problem before they're transformed\n",
    "        into probabiltiies.\n",
    "\n",
    "        They operate on an inherently unlimited scale, being any range of \n",
    "        values whether positive or negative.\n",
    "\n",
    "        In this example these logits will be converted into a score between 0 \n",
    "        and 1 to represent the validity or probability distribution over each\n",
    "        possible output.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        -- Refer to \n",
    "        -- https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n",
    "        \n",
    "        \"\"\"\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the following results you'll notice that the generated text will form to become\n",
    "a string of characters from the BLModel (so  far).\n",
    "\n",
    "However due to there being no clear semantic nor syntactic structure, we can\n",
    "classify that:\n",
    "1. Not well-trained for text generation\n",
    "2. Dataset includes a lot of noise or non-standard text formatting\n",
    "3. Process does not implement any filtering out of non-meaningful characters.\n",
    "\n",
    "The loss value of 4.6497 is slightly over the expected loss value from the following\n",
    "calculation of ln(94) which is equal to 4.543294782.\n",
    "\n",
    "This means that the intial predictions aren't super defuse and possesses some levels\n",
    "of entropy.\n",
    "\n",
    "We are however getting somewhere.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "m = BLModel(vocabularySize)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
