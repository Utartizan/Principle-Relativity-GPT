{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d281536-0ccd-478c-a9b5-9daf67215299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\ubayd\\miniconda3\\lib\\site-packages (3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "403d4f9c-c270-426c-8d81-c3cc8b627189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81a62cdf-9f67-46fa-87b9-1039688eb8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..............................................................................] 73859 / 73859File downloaded as 33663-t.tex\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "\"\"\"\n",
    "Importing (and hence utilising) the wget function to pull data from a file.\n",
    "Which in this case will be the dataset that this Generative Pre-Training Transformer\n",
    "will rely on.\n",
    "\n",
    "This is the 'The Origin and Development of the Quantum Theory' by Max Planck, obtained\n",
    "via the Project Gutenberg website.\n",
    "\"\"\"\n",
    "\n",
    "# Download a file\n",
    "url = \"https://raw.githubusercontent.com/Utartizan/Quantum-Theory-GPT/refs/heads/main/33663-t.tex\"\n",
    "filename = wget.download(url)\n",
    "\n",
    "print(f\"File downloaded as {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12e98e6f-65ec-4962-8c06-152e5652bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('33663-t.tex', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "505d818e-08ad-4c12-abdf-021edf6190ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters is:  73859\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters is: \", len(text))\n",
    "\n",
    "\"\"\"\n",
    "Outputs the amount of characters within the attached dataset. \n",
    "In this case, usually the longer the length of the dataset the\n",
    "better (regarding accuracy) the generative material is.\n",
    "\n",
    "This is due to the quantity in training and validation data that the\n",
    "model can use.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b891a07b-d6cb-4e59-820d-94583c314a2d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %\n",
      "%                                                                         %\n",
      "% The Project Gutenberg EBook of The Origin and Development of the Quantum%\n",
      "% Theory, by Max Planck                                                   %\n",
      "%                                                                         %\n",
      "% This eBook is for the use of anyone anywhere at no cost and with        %\n",
      "% almost no restrictions whatsoever.  You may copy it, give it away or    %\n",
      "% re-use it under the terms of the Project Gutenberg License included     %\n",
      "% with this eBook or online at www.gutenberg.org                          %\n",
      "%                                                                         %\n",
      "%                                                                         %\n",
      "% Title: The Origin and Development of the Quantum Theory                 %\n",
      "%                                                                         %\n",
      "% Author: Ma\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62749558-7ef8-4fc6-ac82-cc7afb9274fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^`abcdefghijklmnopqrstuvwxyz{}~\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Scan the entire file (text) and output:\n",
    "1. All the types of characters utilised\n",
    "2. The quantifiable size of the characters utilised (94)\n",
    "\"\"\"\n",
    "\n",
    "characters = sorted(list(set(text)))\n",
    "vocabularySize = len(characters)\n",
    "print(''.join(characters))\n",
    "print(vocabularySize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45cb1237-c362-4f61-8f04-06d2a4f7824d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 85, 65, 78, 84, 85, 77, 1, 84, 72, 69, 79, 82, 89, 13, 1, 79, 82, 1, 81, 85, 65, 78, 84, 85, 77, 1, 77, 69, 67, 72, 65, 78, 73, 67, 83, 13, 1, 73, 83, 1, 65, 1, 70, 85, 78, 68, 65, 77, 69, 78, 84, 65, 76, 1, 84, 72, 69, 79, 82, 89, 1, 73, 78, 1, 80, 72, 89, 83, 73, 67, 83, 1, 84, 72, 65, 84, 1, 68, 69, 83, 67, 82, 73, 66, 69, 83, 1, 84, 72, 69, 1, 66, 69, 72, 65, 86, 73, 79, 82, 1, 79, 70, 1, 77, 65, 84, 84, 69, 82, 1, 65, 78, 68, 1, 69, 78, 69, 82, 71, 89, 1, 65, 84, 1, 84, 72, 69, 1, 83, 77, 65, 76, 76, 69, 83, 84, 1, 83, 67, 65, 76, 69, 83, 13, 1, 76, 73, 75, 69, 1, 65, 84, 79, 77, 83, 1, 65, 78, 68, 1, 83, 85, 66, 65, 84, 79, 77, 73, 67, 1, 80, 65, 82, 84, 73, 67, 76, 69, 83, 15]\n",
      "Quantum theory, or quantum mechanics, is a fundamental theory in physics that describes the behavior of matter and energy at the smallest scales, like atoms and subatomic particles.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implementing encoding and decoding functions for a character-level tokeniser\n",
    "\n",
    "This essentially assigns each character to its assigned number, for every character\n",
    "in the list is its own number.\n",
    "\n",
    "In this case, the value 50 belongs to the character Q\n",
    "the value 85 belongs to the character U\n",
    "the value 1 belongs to the space character(?)\n",
    "\n",
    "The print functions below exercises the model's ability to both encode and decode\n",
    "a set of charaters, in the form of many words, in the form of a singular sentence \n",
    "accordingly.\n",
    "\"\"\"\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(characters) }\n",
    "itos = { i:ch for i,ch in enumerate(characters) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"Quantum theory, or quantum mechanics, is a fundamental theory in physics that describes the behavior of matter and energy at the smallest scales, like atoms and subatomic particles.\"))\n",
    "print(decode(encode(\"Quantum theory, or quantum mechanics, is a fundamental theory in physics that describes the behavior of matter and energy at the smallest scales, like atoms and subatomic particles.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de39bf0d-05d8-44d3-a078-07b7d151d52d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading torch-2.5.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ubayd\\miniconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ubayd\\miniconda3\\lib\\site-packages (from torch) (3.1.5)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ubayd\\miniconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ubayd\\miniconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.5.1-cp312-cp312-win_amd64.whl (203.0 MB)\n",
      "   ---------------------------------------- 0.0/203.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 2.6/203.0 MB 15.1 MB/s eta 0:00:14\n",
      "    --------------------------------------- 4.7/203.0 MB 11.9 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 6.8/203.0 MB 11.0 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 9.7/203.0 MB 11.4 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 12.3/203.0 MB 11.9 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 15.7/203.0 MB 12.5 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 19.1/203.0 MB 13.1 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 22.3/203.0 MB 13.4 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 25.7/203.0 MB 13.8 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 29.1/203.0 MB 14.1 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 32.5/203.0 MB 14.2 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 35.9/203.0 MB 14.4 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 39.6/203.0 MB 14.6 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 43.0/203.0 MB 14.7 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 46.7/203.0 MB 14.8 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 50.3/203.0 MB 15.0 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 54.0/203.0 MB 15.2 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 57.1/203.0 MB 15.2 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 60.8/203.0 MB 15.3 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 64.2/203.0 MB 15.3 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 67.9/203.0 MB 15.4 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 71.3/203.0 MB 15.5 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 74.7/203.0 MB 15.5 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 77.9/203.0 MB 15.5 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 81.5/203.0 MB 15.5 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 84.1/203.0 MB 15.5 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 87.8/203.0 MB 15.5 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 91.0/203.0 MB 15.5 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 94.4/203.0 MB 15.5 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 97.5/203.0 MB 15.5 MB/s eta 0:00:07\n",
      "   ------------------- ------------------- 101.2/203.0 MB 15.6 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 104.9/203.0 MB 15.6 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 108.0/203.0 MB 15.6 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 111.7/203.0 MB 15.6 MB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 115.3/203.0 MB 15.7 MB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 119.0/203.0 MB 15.7 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 121.6/203.0 MB 15.7 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 125.0/203.0 MB 15.7 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 128.5/203.0 MB 15.7 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 131.6/203.0 MB 15.7 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 134.7/203.0 MB 15.7 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 137.9/203.0 MB 15.6 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 141.6/203.0 MB 15.7 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 145.5/203.0 MB 15.7 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 149.2/203.0 MB 15.8 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 152.3/203.0 MB 15.7 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 156.0/203.0 MB 15.8 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 159.1/203.0 MB 15.8 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 162.3/203.0 MB 15.8 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 165.7/203.0 MB 15.8 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 169.1/203.0 MB 15.8 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 172.5/203.0 MB 15.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 175.9/203.0 MB 15.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 179.6/203.0 MB 15.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 183.0/203.0 MB 15.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 186.1/203.0 MB 15.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 189.5/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 193.2/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 196.6/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  200.0/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------------------------------- 203.0/203.0 MB 12.3 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 3.4/6.2 MB 16.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.2 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 11.9 MB/s eta 0:00:00\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 11.7 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 8.8 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, filelock, torch\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.12.0 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.5.1\n"
     ]
    }
   ],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e8af985-07dc-489a-bf39-c4f4bafbc18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 1, 6, 6, 6, 6, 6, 6, 6])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "trainingData[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a938998-415e-44ea-87ab-3a3ac770a24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([6]) the target: 1\n",
      "when input is tensor([6, 1]) the target: 6\n",
      "when input is tensor([6, 1, 6]) the target: 6\n",
      "when input is tensor([6, 1, 6, 6]) the target: 6\n",
      "when input is tensor([6, 1, 6, 6, 6]) the target: 6\n",
      "when input is tensor([6, 1, 6, 6, 6, 6]) the target: 6\n",
      "when input is tensor([6, 1, 6, 6, 6, 6, 6]) the target: 6\n",
      "when input is tensor([6, 1, 6, 6, 6, 6, 6, 6]) the target: 6\n"
     ]
    }
   ],
   "source": [
    "x = trainingData[:block_size]\n",
    "y = trainingData[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52684261-f666-4613-999d-1dbbde268b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[68, 80, 79, 73, 78, 84,  1, 79],\n",
      "        [69, 76, 69, 67, 84, 82, 79, 78],\n",
      "        [72, 69,  1, 68, 69, 70, 69, 67],\n",
      "        [71, 65, 84, 79, 82, 83,  1, 87]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[80, 79, 73, 78, 84,  1, 79, 70],\n",
      "        [76, 69, 67, 84, 82, 79, 78, 73],\n",
      "        [69,  1, 68, 69, 70, 69, 67, 84],\n",
      "        [65, 84, 79, 82, 83,  1, 87, 72]])\n",
      "----\n",
      "when input is [68] the target: 80\n",
      "when input is [68, 80] the target: 79\n",
      "when input is [68, 80, 79] the target: 73\n",
      "when input is [68, 80, 79, 73] the target: 78\n",
      "when input is [68, 80, 79, 73, 78] the target: 84\n",
      "when input is [68, 80, 79, 73, 78, 84] the target: 1\n",
      "when input is [68, 80, 79, 73, 78, 84, 1] the target: 79\n",
      "when input is [68, 80, 79, 73, 78, 84, 1, 79] the target: 70\n",
      "when input is [69] the target: 76\n",
      "when input is [69, 76] the target: 69\n",
      "when input is [69, 76, 69] the target: 67\n",
      "when input is [69, 76, 69, 67] the target: 84\n",
      "when input is [69, 76, 69, 67, 84] the target: 82\n",
      "when input is [69, 76, 69, 67, 84, 82] the target: 79\n",
      "when input is [69, 76, 69, 67, 84, 82, 79] the target: 78\n",
      "when input is [69, 76, 69, 67, 84, 82, 79, 78] the target: 73\n",
      "when input is [72] the target: 69\n",
      "when input is [72, 69] the target: 1\n",
      "when input is [72, 69, 1] the target: 68\n",
      "when input is [72, 69, 1, 68] the target: 69\n",
      "when input is [72, 69, 1, 68, 69] the target: 70\n",
      "when input is [72, 69, 1, 68, 69, 70] the target: 69\n",
      "when input is [72, 69, 1, 68, 69, 70, 69] the target: 67\n",
      "when input is [72, 69, 1, 68, 69, 70, 69, 67] the target: 84\n",
      "when input is [71] the target: 65\n",
      "when input is [71, 65] the target: 84\n",
      "when input is [71, 65, 84] the target: 79\n",
      "when input is [71, 65, 84, 79] the target: 82\n",
      "when input is [71, 65, 84, 79, 82] the target: 83\n",
      "when input is [71, 65, 84, 79, 82, 83] the target: 1\n",
      "when input is [71, 65, 84, 79, 82, 83, 1] the target: 87\n",
      "when input is [71, 65, 84, 79, 82, 83, 1, 87] the target: 72\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(6969)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = trainingData if split == 'train' else validationData\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "588b6b5f-77f8-4df7-9d5c-3c479deb4eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[68, 80, 79, 73, 78, 84,  1, 79],\n",
      "        [69, 76, 69, 67, 84, 82, 79, 78],\n",
      "        [72, 69,  1, 68, 69, 70, 69, 67],\n",
      "        [71, 65, 84, 79, 82, 83,  1, 87]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9b6624a-6c7b-4ada-9f63-d3070a2f4369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 94])\n",
      "tensor(4.6497, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "y%QxF\n",
      "jEK#CkaW-7yGKHTk4U.[@nACHrh%dJCGn6mnGlzN#nHk.h;I=uzAJ~I'(8oVP<9,I.:@Qv#T\"m uyf>Xiu:\n",
      "@dJF@8TUPz\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(6969)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Creating the definition for the Model (BLModel), essentially creating an embedding\n",
    "layer where each token is assigned/mapped to a vector of the same identity as the\n",
    "vocabulary.\n",
    "\n",
    "This is to allow each token to represent a learnable table where each entry \n",
    "will correspond to X logits for the corresponding token.\n",
    "\"\"\"\n",
    "\n",
    "class BLModel(nn.Module):\n",
    "    def __init__(self, vocabularySize):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocabularySize, vocabularySize)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Logits are unnormalised predictions that would be output by this model\n",
    "        for each class in a classification problem before they're transformed\n",
    "        into probabiltiies.\n",
    "\n",
    "        They operate on an inherently unlimited scale, being any range of \n",
    "        values whether positive or negative.\n",
    "\n",
    "        In this example these logits will be converted into a score between 0 \n",
    "        and 1 to represent the validity or probability distribution over each\n",
    "        possible output.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        -- Refer to \n",
    "        -- https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n",
    "        \n",
    "        \"\"\"\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, ma x_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the following results you'll notice that the generated text will form to become\n",
    "a string of characters from the BLModel (so  far).\n",
    "\n",
    "However due to there being no clear semantic nor syntactic structure, we can\n",
    "classify that:\n",
    "1. Not well-trained for text generation\n",
    "2. Dataset includes a lot of noise or non-standard text formatting\n",
    "3. Process does not implement any filtering out of non-meaningful characters.\n",
    "\n",
    "The loss value of 4.6497 is slightly over the expected loss value from the following\n",
    "calculation of ln(94) which is equal to 4.543294782.\n",
    "\n",
    "This means that the intial predictions aren't super defuse and possesses some levels\n",
    "of entropy.\n",
    "\n",
    "We are however getting somewhere.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "m = BLModel(vocabularySize)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
